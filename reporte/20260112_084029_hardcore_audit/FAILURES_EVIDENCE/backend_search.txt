from __future__ import annotations

import logging
import time
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterable, List, Optional, Set, TypeVar

from opensearchpy import OpenSearch, RequestsHttpConnection

from .config import settings

logger = logging.getLogger(__name__)

T = TypeVar("T")


def _build_client_kwargs() -> Dict[str, Any]:
    use_ssl = settings.opensearch_url.startswith("https")
    kwargs: Dict[str, Any] = {
        "hosts": [settings.opensearch_url],
        "http_compress": True,
        "use_ssl": use_ssl,
        "connection_class": RequestsHttpConnection,
    }

    kwargs["verify_certs"] = bool(use_ssl and settings.opensearch_verify_certs)

    if settings.opensearch_ca_file:
        kwargs["ca_certs"] = settings.opensearch_ca_file

    if settings.opensearch_client_certfile and settings.opensearch_client_keyfile:
        kwargs["client_cert"] = settings.opensearch_client_certfile
        kwargs["client_key"] = settings.opensearch_client_keyfile

    return kwargs


def get_client() -> OpenSearch:
    return OpenSearch(**_build_client_kwargs())


client = get_client()
_ensured_indices: Set[str] = set()
_ensured_aliases: Set[str] = set()


def _retry_operation(action: Callable[[], T]) -> T:
    last_exc: Optional[Exception] = None
    for attempt in range(settings.opensearch_max_retries):
        try:
            return action()
        except Exception as exc:  # noqa: BLE001
            last_exc = exc
            if attempt + 1 >= settings.opensearch_max_retries:
                raise
            backoff = settings.opensearch_retry_backoff_seconds * (2**attempt)
            logger.warning(
                "OpenSearch call failed (attempt %s/%s): %s; retrying in %.2fs",
                attempt + 1,
                settings.opensearch_max_retries,
                exc,
                backoff,
            )
            time.sleep(backoff)
    assert False, f"Unreachable state (last exception: {last_exc})"


EVENTS_INDEX_ALIAS = "events"
ALERTS_INDEX_ALIAS = "alerts"
EVENTS_V2_INDEX = "events-v2"
ALERTS_V2_INDEX = "alerts-v2"

EVENT_V2_MAPPINGS = {
    "mappings": {
        "properties": {
            "event_id": {"type": "keyword"},
            "agent_id": {"type": "integer"},
            "timestamp": {"type": "date"},
            "received_time": {"type": "date"},
            "severity": {"type": "keyword"},
            "event_type": {"type": "keyword"},
            "category": {"type": "keyword"},
            "source": {"type": "keyword"},
            "message": {"type": "text"},
            "correlation_id": {"type": "keyword"},
            "raw_ref": {"type": "keyword"},
            "hostname": {"type": "keyword"},
            "username": {"type": "keyword"},
            "process_name": {"type": "keyword"},
            "action": {"type": "keyword"},
            "url": {"type": "keyword"},
            "domain": {"type": "keyword"},
            "src_ip": {"type": "ip"},
            "dst_ip": {"type": "ip"},
            "sha256": {"type": "keyword"},
            "file_path": {"type": "keyword"},
            "ioc_type": {"type": "keyword"},
            "ioc_value": {"type": "keyword"},
            "sensor_name": {"type": "keyword"},
            "details": {"type": "object", "enabled": False},
        }
    }
}
ALERT_V2_MAPPINGS = {
    "mappings": {
        "properties": {
            "alert_id": {"type": "keyword"},
            "title": {"type": "text"},
            "severity": {"type": "keyword"},
            "status": {"type": "keyword"},
            "category": {"type": "keyword"},
            "source": {"type": "keyword"},
            "timestamp": {"type": "date"},
            "correlation_id": {"type": "keyword"},
            "hostname": {"type": "keyword"},
            "username": {"type": "keyword"},
            "ioc_type": {"type": "keyword"},
            "ioc_value": {"type": "keyword"},
            "details": {"type": "object", "enabled": False},
        }
    }
}
NETWORK_EVENT_MAPPINGS = {
    "mappings": {
        "properties": {
            "ts": {"type": "date"},
            "tenant_id": {"type": "keyword"},
            "source": {"type": "keyword"},
            "event_type": {"type": "keyword"},
            "src_ip": {"type": "ip"},
            "src_port": {"type": "integer"},
            "dst_ip": {"type": "ip"},
            "dst_port": {"type": "integer"},
            "proto": {"type": "keyword"},
            "direction": {"type": "keyword"},
            "sensor_name": {"type": "keyword"},
            "signature": {"type": "keyword"},
            "category": {"type": "keyword"},
            "severity": {"type": "integer"},
            "flow_id": {"type": "keyword"},
            "uid": {"type": "keyword"},
            "community_id": {"type": "keyword"},
            "http": {"type": "object", "enabled": True},
            "dns": {"type": "object", "enabled": True},
            "tls": {"type": "object", "enabled": True},
            "tags": {"type": "keyword"},
            "raw": {"type": "object", "enabled": False},
        }
    }
}
RAW_MAPPINGS = {
        "mappings": {
            "properties": {
                "raw_id": {"type": "keyword"},
                "received_time": {"type": "date"},
                "source": {"type": "keyword"},
                "correlation_id": {"type": "keyword"},
                "collector_id": {"type": "keyword"},
                "tenant_id": {"type": "keyword"},
                "raw_payload": {"type": "object", "enabled": True},
                "transport_meta": {"type": "object", "enabled": True},
                "parse_status": {"type": "keyword"},
            }
        }
    }
DLQ_MAPPINGS = {
    "mappings": {
        "properties": {
            "dlq_id": {"type": "keyword"},
            "time": {"type": "date"},
            "source": {"type": "keyword"},
            "raw_ref": {"type": "keyword"},
            "error_stage": {"type": "keyword"},
            "error_code": {"type": "keyword"},
            "error_detail": {"type": "text"},
            "replay_count": {"type": "integer"},
            "last_replay_time": {"type": "date"},
        }
    }
}


def _ensure_index(index: str, mappings: Dict[str, Any]) -> None:
    if index in _ensured_indices:
        return
    exists = _retry_operation(lambda: client.indices.exists(index=index))
    if not exists:
        _retry_operation(lambda: client.indices.create(index=index, body=mappings))
    _ensured_indices.add(index)


def _ensure_alias(alias: str, index: str) -> None:
    if alias in _ensured_aliases:
        return
    exists = _retry_operation(lambda: client.indices.exists_alias(name=alias))
    if exists:
        aliases = _retry_operation(lambda: client.indices.get_alias(name=alias))
        if index in aliases and alias in aliases[index].get("aliases", {}):
            _ensured_aliases.add(alias)
            return
        actions = []
        for existing_index in aliases:
            actions.append({"remove": {"index": existing_index, "alias": alias}})
        actions.append({"add": {"index": index, "alias": alias}})
        _retry_operation(lambda: client.indices.update_aliases(body={"actions": actions}))
    else:
        _retry_operation(lambda: client.indices.put_alias(index=index, name=alias))
    _ensured_aliases.add(alias)


def ensure_indices() -> None:
    _ensure_index(EVENTS_V2_INDEX, EVENT_V2_MAPPINGS)
    _ensure_index(ALERTS_V2_INDEX, ALERT_V2_MAPPINGS)
    _ensure_alias(EVENTS_INDEX_ALIAS, EVENTS_V2_INDEX)
    _ensure_alias(ALERTS_INDEX_ALIAS, ALERTS_V2_INDEX)


def index_event(doc: Dict[str, object]) -> None:
    _retry_operation(lambda: client.index(index=EVENTS_INDEX_ALIAS, document=doc))


def index_alert(doc: Dict[str, object]) -> None:
    _retry_operation(lambda: client.index(index=ALERTS_INDEX_ALIAS, document=doc))


def index_raw_event(doc: Dict[str, object]) -> None:
    index = _index_for_date("raw-events", doc.get("received_time"))
    try:
        _ensure_index(index, RAW_MAPPINGS)
        _retry_operation(lambda: client.index(index=index, document=doc))
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to index raw event %s: %s", doc.get("raw_id"), exc)


def index_dlq_event(doc: Dict[str, object]) -> None:
    index = _index_for_date("dlq-events", doc.get("time"))
    try:
        _ensure_index(index, DLQ_MAPPINGS)
        _retry_operation(lambda: client.index(index=index, document=doc))
    except Exception as exc:  # noqa: BLE001
        logger.warning("Failed to index DLQ event %s: %s", doc.get("dlq_id"), exc)


def _index_for_date(prefix: str, date_value: object) -> str:
    if isinstance(date_value, str):
        try:
            parsed = datetime.fromisoformat(date_value.replace("Z", "+00:00"))
        except ValueError:
            parsed = datetime.now(timezone.utc)
    elif isinstance(date_value, datetime):
        parsed = date_value
    else:
        parsed = datetime.now(timezone.utc)
    return f"{prefix}-{parsed.strftime('%Y.%m.%d')}"


def bulk_index_network_events(events: List[Dict[str, object]]) -> None:
    if not events:
        return
    batches: Dict[str, List[Dict[str, object]]] = {}
